{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping & Databasing The New Yorker's 'Tables For Two'\n",
    "Last Updated: 10/31/2016\n",
    "\n",
    "### The code in this notebook does three things:\n",
    "1. Scrape The New Yorker's Tables For Two restaurant reviews\n",
    "2. Insert important information for each restaurant into a sqlite database\n",
    "3. Generate a list of 'markers' so we can visualize these locations on the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's import some useful packages\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import pandas\n",
    "from geopy.geocoders import Nominatim\n",
    "import sqlite3 as lite\n",
    "geolocator = Nominatim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping The New Yorker (for the very first time)\n",
    "\n",
    "Starting with the most recent restaurants, each 'page' contains 10 restaurants.\n",
    "We grab the links, add them to our list, then move on to the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### iterate through all pages of The New Yorker tables-for-two history                                                                            \n",
    "step = 1\n",
    "RestLinks = []\n",
    "while step < 80:\n",
    "    page = 'http://www.newyorker.com/magazine/tables-for-two/page/' + str(step)\n",
    "\n",
    "    r = urllib.request.urlopen(page).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "        \n",
    "    #return a list of lists of restaurants from each page\n",
    "    info = soup.findAll('a', {'itemprop':'name'})\n",
    "    \n",
    "    #extract only the link itself\n",
    "    for link in info:\n",
    "        # add links to RestLinks list:                                                                               \n",
    "        RestLinks.append(link.get('href'))\n",
    "\n",
    "    step +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many restaurants in total? What does each entry look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805 total restaurants\n",
      "http://www.newyorker.com/magazine/2016/11/07/dining-from-a-simpler-time\n",
      "http://www.newyorker.com/magazine/2016/10/31/the-lucky-bees-unconventional-thai-street-food\n",
      "http://www.newyorker.com/magazine/2016/10/24/pondicheris-nirvanic-dishes\n",
      "http://www.newyorker.com/magazine/2016/10/17/1633-strange-and-wonderful-on-the-upper-east-side\n",
      "http://www.newyorker.com/magazine/2016/10/10/a-disneyland-of-organic-delights-at-olmsted\n",
      "http://www.newyorker.com/magazine/2016/10/03/dining-for-the-modern-herbivore\n"
     ]
    }
   ],
   "source": [
    "print('%d total restaurants' % len(RestLinks))\n",
    "for i in range(0,6):\n",
    "    print(RestLinks[i].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, get information from each restaurant-specific page\n",
    "\n",
    "For each restaurant in RestLinks, we need to beautiful soup and save:\n",
    "1. Restaurant name\n",
    "2. Street Address\n",
    "3. Phone number\n",
    "4. Text of the review\n",
    "5. Date of the article \n",
    "\n",
    "Lastly, we use Google's geolocator API to extract the latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's save our results to a .txt file, just in case\n",
    "T4Trests = open('T4T_restaurant_db_102816.txt', 'w')\n",
    "\n",
    "with open('key.txt') as f:                                    \n",
    "    key = f.readline()\n",
    "\n",
    "lead = 'https://maps.googleapis.com/maps/api/geocode/json?address='\n",
    "\n",
    "info = []\n",
    "count = 1\n",
    "for article in RestLinks:                                                                                                     \n",
    "    r = urllib.request.urlopen(article).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    # we will also be adding our info here -- to put into a sqlite database\n",
    "    rest_info = []\n",
    "\n",
    "    # grab restaurant name(which may be different from name given in link)\n",
    "    name = soup.findAll('h1', {'itemprop':'headline'})[0].string\n",
    "\n",
    "    # grab street and telephone addresses\n",
    "    street = \"null\"  \n",
    "    tel = \"null\"\n",
    "    lat = \"null\"\n",
    "    lng = \"null\"\n",
    "\n",
    "    # use google's geocode API to look up latitude and longitude\n",
    "    try:\n",
    "        addresses = soup.findAll('h2', {'itemprop':'alternativeHeadline'})[0].string\n",
    "        street = addresses.split('(')[0]\n",
    "        street_split = street.replace(\" \", \"+\")\n",
    "        place = street_split + \",New+York+City,+NY\"\n",
    "        query = (lead + place + key).strip()\n",
    "        resp_json_payload = requests.get(query).json()\n",
    "        \n",
    "        lat = (str(resp_json_payload['results'][0]['geometry']['location']['lat']))\n",
    "        lng = (str(resp_json_payload['results'][0]['geometry']['location']['lng']))\n",
    "        tel = addresses.split('(')[1].strip(')')\n",
    "    except:\n",
    "        pass \n",
    "\n",
    "\n",
    "    # grab article date\n",
    "    date = article.split('/')\n",
    "    dt = date[4] + \"/\" + date[5] + \"/\" + date[6]\n",
    "\n",
    "    #grab text for later meta-analysis\n",
    "    div = soup.find_all('div',{'itemprop': 'articleBody'})\n",
    "    for tag in div:\n",
    "        text = tag.find_all('p')\n",
    "    text_sum = \"\"\n",
    "    for p in text:\n",
    "        text_sum += str(p)\n",
    "\n",
    "    rest_info = (count, name, article, street, str(lat), str(lng), tel, dt, text_sum)\n",
    "\n",
    "    # save our info in memory for writing to database\n",
    "    info.append(rest_info)\n",
    "    # write results to a text file\n",
    "    T4Trests.write(article + \"\\t\" + name + \"\\t\" + street + \"\\t\" + str(lat) + \"\\t\"+ str(lng) + \"\\t\" + tel + \"\\t\"+ dt + \"\\t\"+ text_sum + \"\\n\")\n",
    "    count +=1\n",
    "    \n",
    "T4Trests.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Insert important information for each restaurant into a sqlite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new database, T4T_070916.db that contains the table Rest_inf\n",
    "\n",
    "con = lite.connect('T4T_070916.db')\n",
    "\n",
    "with con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute(\"CREATE TABLE Rest_inf(Id INTEGER PRIMARY KEY, Name TEXT, Article TEXT, Street TEXT, Latitude FLOAT, Longitude FLOAT, Telephone TEXT, Date TEXT, Text TEXT)\");\n",
    "    cur.executemany(\"INSERT INTO Rest_inf VALUES(?,?,?,?,?,?,?,?,?)\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a list of 'markers' so we can visualize these locations on the map\n",
    "\n",
    "Google's html (var locations) works best with a list of lists, where each list contains [name, lat, lng, index]\n",
    "Currently, I am writing my list of lists to a .txt file, then copy and pasting into 'map.html' document\n",
    "\n",
    "Would also like to update this to have:\n",
    "1. Links to the review from the New Yorker (DONE)\n",
    "2. Tags RE: food type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "gmaps = []\n",
    "with con:\n",
    "    for row in cur.execute(\"SELECT Name, Latitude, Longitude, Id, Article FROM Rest_inf ORDER BY Date desc\"):\n",
    "        output.append(row)\n",
    "\n",
    "for entry in output:\n",
    "    try:\n",
    "        lat = float(entry[1])\n",
    "        lng = float(entry[2])\n",
    "    except:\n",
    "        lat = \"null\"       # a fair number of empty locations :(\n",
    "        lng = \"null\"\n",
    "    link = '<a href=\"' + entry[4] + '\">' + entry[0] + '</a>'\n",
    "    item = [link, lat, lng, int(entry[3])]\n",
    "    gmaps.append(item)\n",
    "    \n",
    "gmaps_list = open('gmaps_list_102916.txt', 'w')\n",
    "gmaps_list.write(str(gmaps))\n",
    "gmaps_list.close()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
